{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3055148138e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import brown, stopwords\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "\n",
    "sents = brown.sents()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "tree = ET.parse('../data/test/subtask1-homographic-test.xml')\n",
    "puncts = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = open(\"../data/test/subtask1-homographic-test.gold\")\n",
    "scores = list()\n",
    "for item in F.readlines():\n",
    "    ans = item.split()[1]\n",
    "    scores.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = list()\n",
    "root = tree.getroot()\n",
    "for text in root.getchildren():\n",
    "    sent = list()\n",
    "    for word in text.getchildren():\n",
    "        sent.append(word.text)\n",
    "    test_sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sent(sent):\n",
    "    sent = [w for w in sent if w.lower() not in stopwords]\n",
    "    sent = [w for w in sent if w[0] not in puncts]\n",
    "    return sent\n",
    "\n",
    "def make_ordered_pairs(sent):\n",
    "    ans = list()\n",
    "    for index, word in enumerate(sent):\n",
    "        if index == len(sent) - 1:\n",
    "            break\n",
    "        temp = index + 1\n",
    "        while(1):\n",
    "            ans.append((sent[index], sent[temp]))\n",
    "            if temp == len(sent) - 1:\n",
    "                break\n",
    "            temp += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('never', 230), ('die', 201), ('OLD', 178), ('get', 101), ('good', 66), ('always', 62), ('name', 57), ('said', 57), ('Tom', 56), ('got', 51)]\n",
      "[(('never', 'die'), 194), (('OLD', 'die'), 178), (('OLD', 'never'), 177), (('never', 'lose'), 40), (('die', 'lose'), 40), (('OLD', 'lose'), 38), (('never', 'get'), 36), (('die', 'get'), 34), (('OLD', 'get'), 33), (('Doctor', 'Next'), 19)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = Counter([])\n",
    "bigrams = Counter([])\n",
    "\n",
    "for sent in test_sents:\n",
    "    sent = filter_sent(sent)\n",
    "    pairs = make_ordered_pairs(sent)\n",
    "    words.update(sent)\n",
    "    bigrams.update(pairs)\n",
    "    \n",
    "print(words.most_common(10))\n",
    "print(bigrams.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating PMI\n",
    "\n",
    "The PMI (Pointwise Mututal Information) is being calculated as:\n",
    "```\n",
    "PMI(w1, w2) = log (P(w1, w2) / (P(w1) * P(w2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def PMI(tup):\n",
    "    try:\n",
    "        val = bigrams[tup] / (words[tup[0]] * words[tup[1]])\n",
    "        return math.log(val)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Threshold\n",
    "\n",
    "The threshold for the difference in the highest PMI and is calculated using the Interquartile Range (IQR). IQR is preferred because it is able to eliminate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7107470400723912\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "\n",
    "IQRs = list()\n",
    "for sent in test_sents:\n",
    "    sent = filter_sent(sent)\n",
    "    pairs = make_ordered_pairs(sent)\n",
    "    PMIs = sorted([PMI(_) for _ in pairs])\n",
    "    l = len(PMIs)\n",
    "    if l > 0:\n",
    "        IQRs.append(sp.iqr(PMIs))\n",
    "\n",
    "IQRs = sorted(IQRs)\n",
    "threshold = np.median(IQRs)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sentences for Pun\n",
    "\n",
    "- Converting each sentence into tokens\n",
    "- Stopword Removal\n",
    "- Generating word pairs preserving word order\n",
    "- Calculating PMI score and checking whether above threshold\n",
    "- Checking whether any word from pair has multiple sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def has_multiple_sense(word):\n",
    "    syns = Counter([_.name().split('.')[0] for _ in wn.synsets(word)])\n",
    "    return syns[word] > 1\n",
    "\n",
    "test_scores = list()\n",
    "for i, sent in enumerate(test_sents):\n",
    "    sent_ = sent\n",
    "    sent = filter_sent(sent)\n",
    "    pairs = make_ordered_pairs(sent)\n",
    "    PMIs = sorted([(_, PMI(_)) for _ in pairs], key=lambda k : k[1])\n",
    "    l = len(PMIs)\n",
    "    if l > 0:\n",
    "        if sp.iqr([v for k, v in PMIs]) > threshold:\n",
    "            if has_multiple_sense(PMIs[-1][0][0]) or has_multiple_sense(PMIs[-1][0][1]):\n",
    "                test_scores.append('1')\n",
    "            else:\n",
    "                test_scores.append('0')\n",
    "        else:\n",
    "            test_scores.append('0')\n",
    "    else:\n",
    "        test_scores.append('0')\n",
    "        \n",
    "print(len(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision, Recall, F1-Score\n",
    "\n",
    "```\n",
    "Precision = TP/TP+FP\n",
    "Recall = TP/TP+FN\n",
    "F1-Score = 2*(Recall*Precision)/(Recall+Precision)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.8181818181818182\n",
      "recall 0.35283136278780336\n",
      "F1-score 0.4930434782608695\n"
     ]
    }
   ],
   "source": [
    "t_scores = Counter([(scores[i], test_scores[i]) for i in range(len(scores))])\n",
    "\n",
    "TP = t_scores[('1', '1')]\n",
    "FP = t_scores[('0', '1')]\n",
    "FN = t_scores[('1', '0')]\n",
    "\n",
    "pre = TP / (TP + FP)\n",
    "re = TP / (TP + FN)\n",
    "f1 = 2 * (re * pre) / (re + pre)\n",
    "\n",
    "print('precision', pre)\n",
    "print('recall', re)\n",
    "print('F1-score', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
